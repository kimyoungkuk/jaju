{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "import scipy.misc\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import tensorflow.contrib.layers as layers\n",
    "# from text_loader import TextLoader\n",
    "\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "\n",
    "from sklearn import metrics\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('celeba-dataset/attr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5064975"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total=202599*25\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.847598</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.818762</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.797713</td>\n",
       "      <td>0.964245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002.jpg</td>\n",
       "      <td>0.854284</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.813673</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.296929</td>\n",
       "      <td>0.963263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.893831</td>\n",
       "      <td>0.857029</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.827042</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.797451</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000004.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.848516</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.828885</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.962808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000005.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.792245</td>\n",
       "      <td>0.965147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000006.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.880379</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.844832</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.789205</td>\n",
       "      <td>0.964984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000007.jpg</td>\n",
       "      <td>0.843310</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.856332</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.876754</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.838772</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000008.jpg</td>\n",
       "      <td>0.853705</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.851993</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.827412</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.796232</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>000009.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.793946</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.794794</td>\n",
       "      <td>0.964834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000010.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.845152</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.797354</td>\n",
       "      <td>0.963577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>000011.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.848233</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.964488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>000012.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.851965</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.880994</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.852404</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>000013.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>000014.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.818295</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.964319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>000015.jpg</td>\n",
       "      <td>0.853583</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.890676</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>000016.jpg</td>\n",
       "      <td>0.854084</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>000017.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.841212</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.810045</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.795435</td>\n",
       "      <td>0.965144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>000018.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.877505</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.875282</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.821901</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.963936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>000019.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.849638</td>\n",
       "      <td>0.843485</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.965751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>000020.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.238068</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.847361</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.875345</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>000021.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.841702</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>000022.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.883911</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.849182</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.799684</td>\n",
       "      <td>0.964311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>000023.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.878591</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.873638</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.809743</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.819269</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>000024.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.848564</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.797825</td>\n",
       "      <td>0.966432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>000025.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.792789</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>000026.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.848584</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.790335</td>\n",
       "      <td>0.964672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>000027.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.855844</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.964523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>000028.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.865686</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.965539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>000029.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.848988</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.823475</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.792263</td>\n",
       "      <td>0.965273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>000030.jpg</td>\n",
       "      <td>0.854084</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202569</th>\n",
       "      <td>202570.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.868697</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.846318</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.787956</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202570</th>\n",
       "      <td>202571.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.873830</td>\n",
       "      <td>0.886399</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.330819</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.864441</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.762974</td>\n",
       "      <td>0.966189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202571</th>\n",
       "      <td>202572.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.845949</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.830474</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.963870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202572</th>\n",
       "      <td>202573.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.904575</td>\n",
       "      <td>0.854878</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.825827</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.819723</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.962915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202573</th>\n",
       "      <td>202574.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.848584</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.790335</td>\n",
       "      <td>0.964672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202574</th>\n",
       "      <td>202575.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.849005</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.965324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202575</th>\n",
       "      <td>202576.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.793271</td>\n",
       "      <td>0.964494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202576</th>\n",
       "      <td>202577.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.848959</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.965082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202577</th>\n",
       "      <td>202578.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.868594</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.844060</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.818499</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.793806</td>\n",
       "      <td>0.965360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202578</th>\n",
       "      <td>202579.jpg</td>\n",
       "      <td>0.859691</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.850327</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.806960</td>\n",
       "      <td>0.964611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202579</th>\n",
       "      <td>202580.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.794916</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.870359</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.842945</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.788476</td>\n",
       "      <td>0.964522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202580</th>\n",
       "      <td>202581.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.247887</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.814552</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.859558</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202581</th>\n",
       "      <td>202582.jpg</td>\n",
       "      <td>0.855618</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.887981</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.810389</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.965646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202582</th>\n",
       "      <td>202583.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.838017</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.790484</td>\n",
       "      <td>0.965396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202583</th>\n",
       "      <td>202584.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.874206</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.847879</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.812143</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.963014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202584</th>\n",
       "      <td>202585.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.792789</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202585</th>\n",
       "      <td>202586.jpg</td>\n",
       "      <td>0.857652</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.848269</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202586</th>\n",
       "      <td>202587.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.848493</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.837767</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.819524</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.963668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202587</th>\n",
       "      <td>202588.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.857552</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.842099</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202588</th>\n",
       "      <td>202589.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.840531</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.789156</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202589</th>\n",
       "      <td>202590.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.895807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202590</th>\n",
       "      <td>202591.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.852762</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.849177</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.797561</td>\n",
       "      <td>0.965888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202591</th>\n",
       "      <td>202592.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.882705</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.846669</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.823509</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.790358</td>\n",
       "      <td>0.965263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202592</th>\n",
       "      <td>202593.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.800968</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.849058</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.828619</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.818340</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.964970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202593</th>\n",
       "      <td>202594.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.830843</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.964984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202594</th>\n",
       "      <td>202595.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.879823</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.850510</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.964351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202595</th>\n",
       "      <td>202596.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.815174</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.799841</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202596</th>\n",
       "      <td>202597.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.847356</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.874454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.850123</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.495064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202597</th>\n",
       "      <td>202598.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.792245</td>\n",
       "      <td>0.965147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202598</th>\n",
       "      <td>202599.jpg</td>\n",
       "      <td>0.230243</td>\n",
       "      <td>0.226421</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.227509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.326709</td>\n",
       "      <td>0.231347</td>\n",
       "      <td>0.875538</td>\n",
       "      <td>0.830110</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>0.799619</td>\n",
       "      <td>0.967261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202599 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_id         0         1         2         3         4  \\\n",
       "0       000001.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "1       000002.jpg  0.854284  0.226421  0.228619  0.230902  0.229227   \n",
       "2       000003.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "3       000004.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "4       000005.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "5       000006.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "6       000007.jpg  0.843310  0.226421  0.228619  0.856332  0.229227   \n",
       "7       000008.jpg  0.853705  0.226421  0.228619  0.851993  0.229227   \n",
       "8       000009.jpg  0.230243  0.226421  0.793946  0.230902  0.229227   \n",
       "9       000010.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "10      000011.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "11      000012.jpg  0.230243  0.226421  0.228619  0.851965  0.229227   \n",
       "12      000013.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "13      000014.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "14      000015.jpg  0.853583  0.226421  0.228619  0.230902  0.229227   \n",
       "15      000016.jpg  0.854084  0.226421  0.228619  0.230902  0.229227   \n",
       "16      000017.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "17      000018.jpg  0.230243  0.226421  0.228619  0.230902  0.877505   \n",
       "18      000019.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "19      000020.jpg  0.230243  0.226421  0.238068  0.230902  0.229227   \n",
       "20      000021.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "21      000022.jpg  0.230243  0.226421  0.228619  0.230902  0.883911   \n",
       "22      000023.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "23      000024.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "24      000025.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "25      000026.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "26      000027.jpg  0.230243  0.226421  0.228619  0.855844  0.229227   \n",
       "27      000028.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "28      000029.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "29      000030.jpg  0.854084  0.226421  0.228619  0.230902  0.229227   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "202569  202570.jpg  0.230243  0.226421  0.228619  0.868697  0.229227   \n",
       "202570  202571.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "202571  202572.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "202572  202573.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "202573  202574.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "202574  202575.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "202575  202576.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "202576  202577.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "202577  202578.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "202578  202579.jpg  0.859691  0.226421  0.228619  0.230902  0.229227   \n",
       "202579  202580.jpg  0.230243  0.226421  0.794916  0.230902  0.229227   \n",
       "202580  202581.jpg  0.230243  0.226421  0.228619  0.230902  0.247887   \n",
       "202581  202582.jpg  0.855618  0.226421  0.228619  0.230902  0.229227   \n",
       "202582  202583.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "202583  202584.jpg  0.230243  0.226421  0.228619  0.230902  0.874206   \n",
       "202584  202585.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "202585  202586.jpg  0.857652  0.226421  0.228619  0.230902  0.229227   \n",
       "202586  202587.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "202587  202588.jpg  0.230243  0.226421  0.228619  0.857552  0.229227   \n",
       "202588  202589.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "202589  202590.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "202590  202591.jpg  0.230243  0.226421  0.228619  0.852762  0.229227   \n",
       "202591  202592.jpg  0.230243  0.226421  0.228619  0.230902  0.882705   \n",
       "202592  202593.jpg  0.230243  0.226421  0.800968  0.230902  0.229227   \n",
       "202593  202594.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "202594  202595.jpg  0.230243  0.226421  0.228619  0.230902  0.879823   \n",
       "202595  202596.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "202596  202597.jpg  0.230243  0.226421  0.228619  0.847356  0.229227   \n",
       "202597  202598.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "202598  202599.jpg  0.230243  0.226421  0.228619  0.230902  0.229227   \n",
       "\n",
       "               5         6         7         8    ...           15        16  \\\n",
       "0       0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.847598   \n",
       "1       0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "2       0.230029  0.228527  0.227724  0.227509    ...     0.893831  0.857029   \n",
       "3       0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.848516   \n",
       "4       0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "5       0.880379  0.228527  0.227724  0.227509    ...     0.228122  0.844832   \n",
       "6       0.230029  0.876754  0.227724  0.227509    ...     0.228122  0.838772   \n",
       "7       0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "8       0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "9       0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.845152   \n",
       "10      0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.848233   \n",
       "11      0.230029  0.880994  0.227724  0.227509    ...     0.228122  0.852404   \n",
       "12      0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "13      0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "14      0.230029  0.228527  0.227724  0.227509    ...     0.890676  0.326709   \n",
       "15      0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "16      0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "17      0.230029  0.228527  0.227724  0.227509    ...     0.875282  0.326709   \n",
       "18      0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.849638   \n",
       "19      0.230029  0.228527  0.847361  0.227509    ...     0.228122  0.326709   \n",
       "20      0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.841702   \n",
       "21      0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.849182   \n",
       "22      0.878591  0.228527  0.227724  0.227509    ...     0.873638  0.326709   \n",
       "23      0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.848564   \n",
       "24      0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "25      0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.848584   \n",
       "26      0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "27      0.865686  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "28      0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.848988   \n",
       "29      0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "...          ...       ...       ...       ...    ...          ...       ...   \n",
       "202569  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "202570  0.873830  0.886399  0.227724  0.227509    ...     0.228122  0.330819   \n",
       "202571  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.845949   \n",
       "202572  0.230029  0.228527  0.227724  0.227509    ...     0.904575  0.854878   \n",
       "202573  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.848584   \n",
       "202574  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.849005   \n",
       "202575  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "202576  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.848959   \n",
       "202577  0.868594  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "202578  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.850327   \n",
       "202579  0.870359  0.228527  0.227724  0.227509    ...     0.228122  0.842945   \n",
       "202580  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "202581  0.230029  0.228527  0.227724  0.227509    ...     0.887981  0.326709   \n",
       "202582  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "202583  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.847879   \n",
       "202584  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "202585  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.848269   \n",
       "202586  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.848493   \n",
       "202587  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "202588  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "202589  0.230029  0.228527  0.227724  0.895807    ...     0.228122  0.326709   \n",
       "202590  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.849177   \n",
       "202591  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.846669   \n",
       "202592  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.849058   \n",
       "202593  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "202594  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.850510   \n",
       "202595  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "202596  0.230029  0.228527  0.227724  0.874454    ...     0.228122  0.850123   \n",
       "202597  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "202598  0.230029  0.228527  0.227724  0.227509    ...     0.228122  0.326709   \n",
       "\n",
       "              17        18        19        20        21        22        23  \\\n",
       "0       0.231347  0.226592  0.231567  0.227144  0.818762  0.226646  0.797713   \n",
       "1       0.231347  0.226592  0.231567  0.227144  0.813673  0.226646  0.296929   \n",
       "2       0.231347  0.226592  0.827042  0.227144  0.236010  0.226646  0.797451   \n",
       "3       0.231347  0.226592  0.828885  0.227144  0.236010  0.226646  0.290901   \n",
       "4       0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.792245   \n",
       "5       0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.789205   \n",
       "6       0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "7       0.231347  0.226592  0.827412  0.227144  0.236010  0.226646  0.796232   \n",
       "8       0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.794794   \n",
       "9       0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.797354   \n",
       "10      0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "11      0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "12      0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "13      0.231347  0.226592  0.231567  0.227144  0.818295  0.226646  0.290901   \n",
       "14      0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "15      0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "16      0.841212  0.226592  0.231567  0.227144  0.810045  0.226646  0.795435   \n",
       "17      0.231347  0.226592  0.231567  0.227144  0.821901  0.226646  0.290901   \n",
       "18      0.843485  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "19      0.231347  0.226592  0.231567  0.875345  0.236010  0.226646  0.290901   \n",
       "20      0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "21      0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.799684   \n",
       "22      0.231347  0.226592  0.231567  0.227144  0.809743  0.226646  0.819269   \n",
       "23      0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.797825   \n",
       "24      0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.792789   \n",
       "25      0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.790335   \n",
       "26      0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "27      0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "28      0.231347  0.226592  0.823475  0.227144  0.236010  0.226646  0.792263   \n",
       "29      0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "202569  0.846318  0.226592  0.231567  0.227144  0.236010  0.226646  0.787956   \n",
       "202570  0.231347  0.864441  0.231567  0.227144  0.236010  0.226646  0.762974   \n",
       "202571  0.231347  0.226592  0.830474  0.227144  0.236010  0.226646  0.290901   \n",
       "202572  0.231347  0.226592  0.825827  0.227144  0.819723  0.226646  0.290901   \n",
       "202573  0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.790335   \n",
       "202574  0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "202575  0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.793271   \n",
       "202576  0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "202577  0.844060  0.226592  0.231567  0.227144  0.818499  0.226646  0.793806   \n",
       "202578  0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.806960   \n",
       "202579  0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.788476   \n",
       "202580  0.814552  0.226592  0.231567  0.859558  0.236010  0.226646  0.290901   \n",
       "202581  0.231347  0.226592  0.231567  0.227144  0.810389  0.226646  0.290901   \n",
       "202582  0.838017  0.226592  0.231567  0.227144  0.236010  0.226646  0.790484   \n",
       "202583  0.231347  0.226592  0.231567  0.227144  0.812143  0.226646  0.290901   \n",
       "202584  0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.792789   \n",
       "202585  0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "202586  0.231347  0.226592  0.837767  0.227144  0.819524  0.226646  0.290901   \n",
       "202587  0.842099  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "202588  0.840531  0.226592  0.231567  0.227144  0.236010  0.226646  0.789156   \n",
       "202589  0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "202590  0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.797561   \n",
       "202591  0.231347  0.226592  0.231567  0.227144  0.823509  0.226646  0.790358   \n",
       "202592  0.231347  0.226592  0.828619  0.227144  0.818340  0.226646  0.290901   \n",
       "202593  0.231347  0.226592  0.830843  0.227144  0.236010  0.226646  0.290901   \n",
       "202594  0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "202595  0.231347  0.226592  0.231567  0.227144  0.815174  0.226646  0.799841   \n",
       "202596  0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.290901   \n",
       "202597  0.231347  0.226592  0.231567  0.227144  0.236010  0.226646  0.792245   \n",
       "202598  0.231347  0.875538  0.830110  0.227144  0.236010  0.226646  0.799619   \n",
       "\n",
       "              24  \n",
       "0       0.964245  \n",
       "1       0.963263  \n",
       "2       0.495064  \n",
       "3       0.962808  \n",
       "4       0.965147  \n",
       "5       0.964984  \n",
       "6       0.495064  \n",
       "7       0.495064  \n",
       "8       0.964834  \n",
       "9       0.963577  \n",
       "10      0.964488  \n",
       "11      0.495064  \n",
       "12      0.495064  \n",
       "13      0.964319  \n",
       "14      0.495064  \n",
       "15      0.495064  \n",
       "16      0.965144  \n",
       "17      0.963936  \n",
       "18      0.965751  \n",
       "19      0.495064  \n",
       "20      0.495064  \n",
       "21      0.964311  \n",
       "22      0.495064  \n",
       "23      0.966432  \n",
       "24      0.495064  \n",
       "25      0.964672  \n",
       "26      0.964523  \n",
       "27      0.965539  \n",
       "28      0.965273  \n",
       "29      0.495064  \n",
       "...          ...  \n",
       "202569  0.495064  \n",
       "202570  0.966189  \n",
       "202571  0.963870  \n",
       "202572  0.962915  \n",
       "202573  0.964672  \n",
       "202574  0.965324  \n",
       "202575  0.964494  \n",
       "202576  0.965082  \n",
       "202577  0.965360  \n",
       "202578  0.964611  \n",
       "202579  0.964522  \n",
       "202580  0.495064  \n",
       "202581  0.965646  \n",
       "202582  0.965396  \n",
       "202583  0.963014  \n",
       "202584  0.495064  \n",
       "202585  0.495064  \n",
       "202586  0.963668  \n",
       "202587  0.495064  \n",
       "202588  0.495064  \n",
       "202589  0.495064  \n",
       "202590  0.965888  \n",
       "202591  0.965263  \n",
       "202592  0.964970  \n",
       "202593  0.964984  \n",
       "202594  0.964351  \n",
       "202595  0.495064  \n",
       "202596  0.495064  \n",
       "202597  0.965147  \n",
       "202598  0.967261  \n",
       "\n",
       "[202599 rows x 26 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.contrib.distributions\n",
    "layers = tf.contrib.layers\n",
    "tfgan = tf.contrib.gan\n",
    "##리턴값 파악필요\n",
    "##아직 conditional 고려안함 가져온 함수만 copnditional 버전\n",
    "def _generator_helper(noise, is_conditional, one_hot_labels, weight_decay, is_training):\n",
    "    \n",
    "    with tf.contrib.framework.arg_scope(\n",
    "        [layers.fully_connected, layers.conv2d_transpose],\n",
    "        activation_fn=tf.nn.relu, normalizer_fn=layers.batch_norm,\n",
    "        weights_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "        with tf.contrib.framework.arg_scope([layers.batch_norm], is_training=is_training):\n",
    "            net = layers.fully_connected(noise, 1024)\n",
    "            print(net)\n",
    "            if is_conditional:\n",
    "                net = tfgan.features.condition_tensor_from_onehot(net, one_hot_labels)\n",
    "            net = layers.fully_connected(net, 4 * 4 * 512)\n",
    "            print(net)\n",
    "            net = tf.reshape(net, [-1, 4, 4, 512])\n",
    "#             print(net)\n",
    "#             res1 = layers.conv2d_transpose(net, 256, [4, 4], stride=2)\n",
    "#             print(res1)\n",
    "#             res2 = layers.conv2d_transpose(res1+net, 128, [4, 4], stride=2)\n",
    "#             print(res2)\n",
    "#             res3 = layers.conv2d_transpose(res2+res1, 64, [4, 4], stride=2)\n",
    "#             print(res3)\n",
    "#             net = layers.conv2d_transpose(res3+res2, 32, [4, 4], stride=2)\n",
    "            print(net)\n",
    "            net = layers.conv2d_transpose(net, 256, [4, 4], stride=2)\n",
    "            print(net)\n",
    "            net = layers.conv2d_transpose(net, 128, [4, 4], stride=2)\n",
    "            print(net)\n",
    "            net = layers.conv2d_transpose(net, 64, [4, 4], stride=2)\n",
    "            print(net)\n",
    "            net = layers.conv2d_transpose(net, 32, [4, 4], stride=2)\n",
    "            print(net)\n",
    "            # Make sure that generator output is in the same range as `inputs`\n",
    "            # ie [-1, 1].\n",
    "            net = layers.conv2d(net, 3, [4, 4], normalizer_fn=None, activation_fn=tf.tanh)\n",
    "            print(net)\n",
    "        return net\n",
    "    \n",
    "def unconditional_generator(noise, weight_decay=2.5e-5, is_training=True):\n",
    "\n",
    "    return _generator_helper(noise, False, None, weight_decay, is_training)\n",
    "\n",
    "def conditional_generator(inputs, weight_decay=2.5e-5, is_training=True):\n",
    "    \n",
    "    noise, one_hot_labels = inputs\n",
    "    return _generator_helper(noise, True, one_hot_labels, weight_decay, is_training)\n",
    "\n",
    "def _discriminator_helper(img, is_conditional, one_hot_labels, weight_decay):\n",
    "#     tf.nn.leaky_relu(tf.layers.batch_normalization(deconv1, training=isTrain), 0.2)\n",
    "    with tf.contrib.framework.arg_scope(\n",
    "        [layers.conv2d, layers.fully_connected],\n",
    "        activation_fn=tf.nn.leaky_relu, normalizer_fn=None,\n",
    "        weights_regularizer=layers.l2_regularizer(weight_decay),\n",
    "        biases_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "        print('@@@')\n",
    "        print(img)\n",
    "        net = layers.fully_connected(img, 32)\n",
    "#         print(net)\n",
    "#         res1 = layers.conv2d(net, 64, [4, 4], stride=2)\n",
    "#         print(res1)\n",
    "#         res2 = layers.conv2d(res1+net, 128, [4, 4], stride=2)\n",
    "#         print(res2)\n",
    "#         res3 = layers.conv2d(res2+res1, 256, [4, 4], stride=2)\n",
    "#         print(res3)\n",
    "#         net = layers.conv2d(res3+res2, 512, [4, 4], stride=2)\n",
    "        print(net)\n",
    "        net = layers.conv2d(net, 64, [4, 4], stride=2)\n",
    "        print(net)\n",
    "        net = layers.conv2d(net, 128, [4, 4], stride=2)\n",
    "        print(net)\n",
    "        net = layers.conv2d(net, 256, [4, 4], stride=2)\n",
    "        print(net)\n",
    "        net = layers.conv2d(net, 512, [4, 4], stride=2)\n",
    "        print(net)\n",
    "        net = layers.flatten(net)\n",
    "        print(net)\n",
    "        if is_conditional:\n",
    "            net = tfgan.features.condition_tensor_from_onehot(net, one_hot_labels)\n",
    "        net = layers.fully_connected(net, 1024, normalizer_fn=layers.layer_norm)\n",
    "        print(net)\n",
    "\n",
    "        return net\n",
    "    \n",
    "def unconditional_discriminator(img, unused_conditioning, weight_decay=2.5e-5):\n",
    "\n",
    "    net = _discriminator_helper(img, False, None, weight_decay)\n",
    "    return layers.linear(net, 1)\n",
    "\n",
    "def conditional_discriminator(img, conditioning, weight_decay=2.5e-5):\n",
    "    \n",
    "    _, one_hot_labels = conditioning\n",
    "    net = _discriminator_helper(img, True, one_hot_labels, weight_decay)\n",
    "    return layers.linear(net, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = tf.contrib.distributions\n",
    "# layers = tf.contrib.layers\n",
    "# tfgan = tf.contrib.gan\n",
    "# ##리턴값 파악필요\n",
    "# ##아직 conditional 고려안함 가져온 함수만 copnditional 버전\n",
    "# def _generator_helper(noise, is_conditional, one_hot_labels, weight_decay, is_training):\n",
    "    \n",
    "#     with tf.contrib.framework.arg_scope(\n",
    "#         [layers.fully_connected, layers.conv2d_transpose],\n",
    "#         activation_fn=tf.nn.relu, normalizer_fn=layers.batch_norm,\n",
    "#         weights_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "#         with tf.contrib.framework.arg_scope([layers.batch_norm], is_training=is_training):\n",
    "#             net = layers.fully_connected(noise, 1024)\n",
    "#             print(net)\n",
    "#             if is_conditional:\n",
    "#                 net = tfgan.features.condition_tensor_from_onehot(net, one_hot_labels)\n",
    "#             net = layers.fully_connected(net, 16 * 16 * 256)\n",
    "#             print(net)\n",
    "#             net = tf.reshape(net, [-1, 16, 16, 256])\n",
    "#             print(net)\n",
    "#             net = layers.conv2d_transpose(net, 64, [4, 4], stride=2)\n",
    "#             print(net)\n",
    "#             net = layers.conv2d_transpose(net, 16, [4, 4], stride=2)\n",
    "#             print(net)\n",
    "#             # Make sure that generator output is in the same range as `inputs`\n",
    "#             # ie [-1, 1].\n",
    "#             net = layers.conv2d(net, 3, [4, 4], normalizer_fn=None, activation_fn=tf.tanh)\n",
    "#             print(net)\n",
    "#         return net\n",
    "    \n",
    "# def unconditional_generator(noise, weight_decay=2.5e-5, is_training=True):\n",
    "\n",
    "#     return _generator_helper(noise, False, None, weight_decay, is_training)\n",
    "\n",
    "# def conditional_generator(inputs, weight_decay=2.5e-5, is_training=True):\n",
    "    \n",
    "#     noise, one_hot_labels = inputs\n",
    "#     return _generator_helper(noise, True, one_hot_labels, weight_decay, is_training)\n",
    "\n",
    "# def _discriminator_helper(img, is_conditional, one_hot_labels, weight_decay):\n",
    "# #     tf.nn.leaky_relu(tf.layers.batch_normalization(deconv1, training=isTrain), 0.2)\n",
    "#     with tf.contrib.framework.arg_scope(\n",
    "#         [layers.conv2d, layers.fully_connected],\n",
    "#         activation_fn=tf.nn.leaky_relu, normalizer_fn=None,\n",
    "#         weights_regularizer=layers.l2_regularizer(weight_decay),\n",
    "#         biases_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "#         print('@@@')\n",
    "#         print(img)\n",
    "#         net = layers.conv2d(img, 64, [4, 4], stride=2)\n",
    "#         print(net)\n",
    "#         net = layers.conv2d(net, 256, [4, 4], stride=2)\n",
    "#         print(net)\n",
    "#         net = layers.flatten(net)\n",
    "#         print(net)\n",
    "#         if is_conditional:\n",
    "#             net = tfgan.features.condition_tensor_from_onehot(net, one_hot_labels)\n",
    "#         net = layers.fully_connected(net, 1024, normalizer_fn=layers.layer_norm)\n",
    "#         print(net)\n",
    "\n",
    "#         return net\n",
    "    \n",
    "# def unconditional_discriminator(img, unused_conditioning, weight_decay=2.5e-5):\n",
    "\n",
    "#     net = _discriminator_helper(img, False, None, weight_decay)\n",
    "#     return layers.linear(net, 1)\n",
    "\n",
    "# def conditional_discriminator(img, conditioning, weight_decay=2.5e-5):\n",
    "    \n",
    "#     _, one_hot_labels = conditioning\n",
    "#     net = _discriminator_helper(img, True, one_hot_labels, weight_decay)\n",
    "#     return layers.linear(net, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_queue(csv_file_name,num_epochs = None):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    for line in open(csv_file_name,'r'):\n",
    "        cols = re.split(',|\\n',line)\n",
    "        train_images.append('celeba-dataset/img_align_celeba/'+cols[0])\n",
    "        # 3rd column is label and needs to be converted to int type\n",
    "        train_labels.append(float(cols[2]))\n",
    "    input_queue = tf.train.slice_input_producer([train_images,train_labels],num_epochs = num_epochs,shuffle = True)\n",
    "\n",
    "    return input_queue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_queue):\n",
    "    image_file = input_queue[0]\n",
    "    label = input_queue[1]\n",
    "    image =  tf.image.decode_jpeg(tf.read_file(image_file),channels=3)\n",
    "\n",
    "    return image,label,image_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_batch(csv_file_name,batch_size):\n",
    "    input_queue = get_input_queue(csv_file_name)\n",
    "    image,label,file_name= read_data(input_queue)\n",
    "    image = tf.image.resize_images(image,[64,64])\n",
    "    image = tf.reshape(image,[64,64,3])\n",
    "\n",
    "    # random image\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "#     image = tf.image.random_brightness(image,max_delta=0.5)\n",
    "#     image = tf.image.random_contrast(image,lower=0.2,upper=2.0)\n",
    "#     image = tf.image.random_hue(image,max_delta=0.08)\n",
    "#     image = tf.image.random_saturation(image,lower=0.2,upper=2.0)\n",
    "\n",
    "    batch_image,batch_label,batch_file = tf.train.batch([image,label,file_name],batch_size=batch_size)\n",
    "    print(batch_image)\n",
    "    print(batch_label)\n",
    "    print(batch_file)\n",
    "    #,enqueue_many=True)\n",
    "    batch_file = tf.reshape(batch_file,[batch_size,1],name='img')\n",
    "\n",
    "    return tf.cast(batch_image,tf.float32),batch_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"batch:0\", shape=(64, 64, 64, 3), dtype=float32)\n",
      "Tensor(\"batch:1\", shape=(64,), dtype=float32)\n",
      "Tensor(\"batch:2\", shape=(64,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "image_batch,label_batch = read_data_batch('celeba-dataset/attr.csv',64) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfgan = tf.contrib.gan\n",
    "\n",
    "def _get_train_input_fn(batch_size, noise_dims, dataset_dir=None,num_threads=4):\n",
    "    def train_input_fn():\n",
    "        with tf.device('/cpu:0'):\n",
    "            ##데이터제작 http://bcho.tistory.com/1178 참고\n",
    "            images,labels = read_data_batch('celeba-dataset/attr.csv',32) \n",
    "        noise = tf.random_normal([batch_size, noise_dims])\n",
    "        return noise, images\n",
    "    return train_input_fn\n",
    "\n",
    "\n",
    "def _get_predict_input_fn(batch_size, noise_dims):\n",
    "    def predict_input_fn():\n",
    "        noise = tf.random_normal([batch_size, noise_dims])\n",
    "        return noise\n",
    "    return predict_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp0t4wyrxx\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_service': None, '_log_step_count_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb4e0b39898>, '_device_fn': None, '_tf_random_seed': None, '_session_config': None, '_model_dir': '/tmp/tmp0t4wyrxx', '_keep_checkpoint_max': 5, '_num_worker_replicas': 1, '_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_task_id': 0, '_task_type': 'worker', '_global_id_in_cluster': 0, '_train_distribute': None, '_master': '', '_is_chief': True, '_evaluation_master': '', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "gan_estimator = tf.contrib.gan.estimator.GANEstimator(\n",
    "    generator_fn=unconditional_generator,\n",
    "    discriminator_fn=unconditional_discriminator,\n",
    "    ##loss함수를 직접 만들어서 함수자체를 넘겨줘야할 수 있음\n",
    "    generator_loss_fn=tf.contrib.gan.losses.wasserstein_generator_loss,\n",
    "    discriminator_loss_fn=tf.contrib.gan.losses.wasserstein_discriminator_loss,\n",
    "    generator_optimizer=tf.train.AdamOptimizer(0.01, 0.5),\n",
    "    discriminator_optimizer=tf.train.AdamOptimizer(0.01, 0.5),\n",
    "    add_summaries=tfgan.estimator.SummaryType.IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"batch:0\", shape=(32, 64, 64, 3), dtype=float32, device=/device:CPU:0)\n",
      "Tensor(\"batch:1\", shape=(32,), dtype=float32, device=/device:CPU:0)\n",
      "Tensor(\"batch:2\", shape=(32,), dtype=string, device=/device:CPU:0)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Tensor(\"Generator/fully_connected/Relu:0\", shape=(32, 1024), dtype=float32)\n",
      "Tensor(\"Generator/fully_connected_1/Relu:0\", shape=(32, 8192), dtype=float32)\n",
      "Tensor(\"Generator/Reshape:0\", shape=(32, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"Generator/Conv2d_transpose/Relu:0\", shape=(32, 8, 8, 256), dtype=float32)\n",
      "Tensor(\"Generator/Conv2d_transpose_1/Relu:0\", shape=(32, 16, 16, 128), dtype=float32)\n",
      "Tensor(\"Generator/Conv2d_transpose_2/Relu:0\", shape=(32, 32, 32, 64), dtype=float32)\n",
      "Tensor(\"Generator/Conv2d_transpose_3/Relu:0\", shape=(32, 64, 64, 32), dtype=float32)\n",
      "Tensor(\"Generator/Conv/Tanh:0\", shape=(32, 64, 64, 3), dtype=float32)\n",
      "@@@\n",
      "Tensor(\"Generator/Conv/Tanh:0\", shape=(32, 64, 64, 3), dtype=float32)\n",
      "Tensor(\"Discriminator/fully_connected/LeakyRelu:0\", shape=(32, 64, 64, 32), dtype=float32)\n",
      "Tensor(\"Discriminator/Conv/LeakyRelu:0\", shape=(32, 32, 32, 64), dtype=float32)\n",
      "Tensor(\"Discriminator/Conv_1/LeakyRelu:0\", shape=(32, 16, 16, 128), dtype=float32)\n",
      "Tensor(\"Discriminator/Conv_2/LeakyRelu:0\", shape=(32, 8, 8, 256), dtype=float32)\n",
      "Tensor(\"Discriminator/Conv_3/LeakyRelu:0\", shape=(32, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"Discriminator/Flatten/flatten/Reshape:0\", shape=(32, 8192), dtype=float32)\n",
      "Tensor(\"Discriminator/fully_connected_1/LeakyRelu:0\", shape=(32, 1024), dtype=float32)\n",
      "@@@\n",
      "Tensor(\"batch:0\", shape=(32, 64, 64, 3), dtype=float32, device=/device:CPU:0)\n",
      "Tensor(\"Discriminator_1/fully_connected/LeakyRelu:0\", shape=(32, 64, 64, 32), dtype=float32)\n",
      "Tensor(\"Discriminator_1/Conv/LeakyRelu:0\", shape=(32, 32, 32, 64), dtype=float32)\n",
      "Tensor(\"Discriminator_1/Conv_1/LeakyRelu:0\", shape=(32, 16, 16, 128), dtype=float32)\n",
      "Tensor(\"Discriminator_1/Conv_2/LeakyRelu:0\", shape=(32, 8, 8, 256), dtype=float32)\n",
      "Tensor(\"Discriminator_1/Conv_3/LeakyRelu:0\", shape=(32, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"Discriminator_1/Flatten/flatten/Reshape:0\", shape=(32, 8192), dtype=float32)\n",
      "Tensor(\"Discriminator_1/fully_connected_1/LeakyRelu:0\", shape=(32, 1024), dtype=float32)\n",
      "INFO:tensorflow:Summary name Generator/fully_connected/weights:0 is illegal; using Generator/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/fully_connected/BatchNorm/beta:0 is illegal; using Generator/fully_connected/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/fully_connected_1/weights:0 is illegal; using Generator/fully_connected_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/fully_connected_1/BatchNorm/beta:0 is illegal; using Generator/fully_connected_1/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/Conv2d_transpose/weights:0 is illegal; using Generator/Conv2d_transpose/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/Conv2d_transpose/BatchNorm/beta:0 is illegal; using Generator/Conv2d_transpose/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/Conv2d_transpose_1/weights:0 is illegal; using Generator/Conv2d_transpose_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/Conv2d_transpose_1/BatchNorm/beta:0 is illegal; using Generator/Conv2d_transpose_1/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/Conv2d_transpose_2/weights:0 is illegal; using Generator/Conv2d_transpose_2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/Conv2d_transpose_2/BatchNorm/beta:0 is illegal; using Generator/Conv2d_transpose_2/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/Conv2d_transpose_3/weights:0 is illegal; using Generator/Conv2d_transpose_3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/Conv2d_transpose_3/BatchNorm/beta:0 is illegal; using Generator/Conv2d_transpose_3/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/Conv/weights:0 is illegal; using Generator/Conv/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/Conv/biases:0 is illegal; using Generator/Conv/biases_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/fully_connected/weights:0 is illegal; using Discriminator/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/fully_connected/biases:0 is illegal; using Discriminator/fully_connected/biases_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/Conv/weights:0 is illegal; using Discriminator/Conv/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/Conv/biases:0 is illegal; using Discriminator/Conv/biases_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/Conv_1/weights:0 is illegal; using Discriminator/Conv_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/Conv_1/biases:0 is illegal; using Discriminator/Conv_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/Conv_2/weights:0 is illegal; using Discriminator/Conv_2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/Conv_2/biases:0 is illegal; using Discriminator/Conv_2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/Conv_3/weights:0 is illegal; using Discriminator/Conv_3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/Conv_3/biases:0 is illegal; using Discriminator/Conv_3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/fully_connected_1/weights:0 is illegal; using Discriminator/fully_connected_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/fully_connected_1/LayerNorm/beta:0 is illegal; using Discriminator/fully_connected_1/LayerNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/fully_connected_1/LayerNorm/gamma:0 is illegal; using Discriminator/fully_connected_1/LayerNorm/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/fully_connected_2/weights:0 is illegal; using Discriminator/fully_connected_2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/fully_connected_2/biases:0 is illegal; using Discriminator/fully_connected_2/biases_0 instead.\n",
      "WARNING:tensorflow:update_ops in create_train_op does not contain all the  update_ops in GraphKeys.UPDATE_OPS\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp0t4wyrxx/model.ckpt.\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:loss = -13.815141, step = 1\n",
      "INFO:tensorflow:global_step/sec: 5.44162\n",
      "INFO:tensorflow: (18.377 sec)\n",
      "INFO:tensorflow:loss = -2924.7017, step = 101 (18.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.96014\n",
      "INFO:tensorflow: (14.368 sec)\n",
      "INFO:tensorflow:loss = -10916.953, step = 201 (14.368 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.78154\n",
      "INFO:tensorflow: (14.746 sec)\n",
      "INFO:tensorflow:loss = -24073.695, step = 301 (14.746 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.88269\n",
      "INFO:tensorflow: (14.529 sec)\n",
      "INFO:tensorflow:loss = -44293.312, step = 401 (14.529 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into /tmp/tmp0t4wyrxx/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: -68809.28.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.gan.python.estimator.python.gan_estimator_impl.GANEstimator at 0x7fb4bb032b38>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensors_to_log = {'img':'img'}\n",
    "tensors_to_log = {}\n",
    "logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=100)\n",
    "\n",
    "##리턴값 파악필요\n",
    "train_input_fn = _get_train_input_fn(32, 256)\n",
    "gan_estimator.train(train_input_fn, max_steps=500, hooks=[logging_hook])\n",
    "\n",
    "\n",
    "#   # Nicely tile.\n",
    "# image_rows = [np.concatenate(predictions[i:i+6], axis=0) for i in range(0, 36, 6)]\n",
    "# tiled_image = np.concatenate(image_rows, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Tensor(\"Generator/fully_connected/Relu:0\", shape=(1, 1024), dtype=float32)\n",
      "Tensor(\"Generator/fully_connected_1/Relu:0\", shape=(1, 8192), dtype=float32)\n",
      "Tensor(\"Generator/Reshape:0\", shape=(1, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"Generator/Conv2d_transpose/Relu:0\", shape=(1, 8, 8, 256), dtype=float32)\n",
      "Tensor(\"Generator/Conv2d_transpose_1/Relu:0\", shape=(1, 16, 16, 128), dtype=float32)\n",
      "Tensor(\"Generator/Conv2d_transpose_2/Relu:0\", shape=(1, 32, 32, 64), dtype=float32)\n",
      "Tensor(\"Generator/Conv2d_transpose_3/Relu:0\", shape=(1, 64, 64, 32), dtype=float32)\n",
      "Tensor(\"Generator/Conv/Tanh:0\", shape=(1, 64, 64, 3), dtype=float32)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp0t4wyrxx/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "<class 'generator'>\n",
      "<generator object Estimator.predict at 0x7fb4d85f3ba0>\n",
      "<class 'numpy.ndarray'>\n",
      "(64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# # Run inference.\n",
    "predict_input_fn = _get_predict_input_fn(1, 256)\n",
    "predictions = gan_estimator.predict(predict_input_fn)\n",
    "# predictions = [prediction_iterable.next() for _ in xrange(36)]\n",
    "# y_predicted = np.array(list(predictions))\n",
    "predictions\n",
    "n=0\n",
    "for x in predictions:\n",
    "    print(type(predictions))\n",
    "    print(predictions)\n",
    "    print(type(x))\n",
    "    print(x.shape)\n",
    "#     print(x)\n",
    "    predictions=x\n",
    "    if(n==0):\n",
    "        break\n",
    "    n=n+1\n",
    "    \n",
    "# print(predictions)\n",
    "\n",
    "# y_predicted = np.array(list(predictions for _ in xrange(36)))\n",
    "# print(y_predicted)\n",
    "# print(y_predicted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb4bb32e320>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scipy.misc.imsave('outfile.jpg', predictions)\n",
    "outimg = imread('outfile.jpg')\n",
    "plt.imshow(outimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generator_fn(x, y_label,):\n",
    "#     w_init = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
    "#     b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "#     # concat layer\n",
    "#     cat1 = tf.concat([x, y_label], 3)\n",
    "\n",
    "#     # 1st hidden layer\n",
    "#     deconv1 = tf.layers.conv2d_transpose(cat1, 256, [7, 7], strides=(1, 1), padding='valid', kernel_initializer=w_init, bias_initializer=b_init)\n",
    "#     lrelu1 = tf.nn.leaky_relu(tf.layers.batch_normalization(deconv1, training=isTrain), 0.2)\n",
    "\n",
    "#     # 2nd hidden layer\n",
    "#     deconv2 = tf.layers.conv2d_transpose(lrelu1, 128, [5, 5], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init)\n",
    "#     lrelu2 = tf.nn.leaky_relu(tf.layers.batch_normalization(deconv2, training=isTrain), 0.2)\n",
    "\n",
    "#     # output layer\n",
    "#     deconv3 = tf.layers.conv2d_transpose(lrelu2, 1, [5, 5], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init)\n",
    "#     o = tf.nn.tanh(deconv3)\n",
    "\n",
    "#     return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def discriminator_fn(x, y_fill):\n",
    "#     w_init = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
    "#     b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "#     # concat layer\n",
    "#     cat1 = tf.concat([x, y_fill], 3)\n",
    "\n",
    "#     # 1st hidden layer\n",
    "#     conv1 = tf.layers.conv2d(cat1, 128, [5, 5], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init)\n",
    "#     lrelu1 = tf.nn.leaky_relu(conv1, 0.2)\n",
    "\n",
    "#     # 2nd hidden layer\n",
    "#     conv2 = tf.layers.conv2d(lrelu1, 256, [5, 5], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init)\n",
    "#     lrelu2 = tf.nn.leaky_relu(tf.layers.batch_normalization(conv2, training=isTrain), 0.2)\n",
    "\n",
    "#     # output layer\n",
    "#     conv3 = tf.layers.conv2d(lrelu2, 1, [7, 7], strides=(1, 1), padding='valid', kernel_initializer=w_init)\n",
    "#     o = tf.nn.sigmoid(conv3)\n",
    "\n",
    "#     return o, conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gan_estimator.evaluate(eval_input_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = np.array([x for x in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
